{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_excel('./indeed_results_pp_2020-04-27.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = df[(df['Title_New'].notnull()) & ((df['Education_Tenth'] == 1) | (df['Education_Twelvth'] == 1) | (df['Education_Bachelors'] == 1) | (df['Education_Masters'] == 1) | (df['Education_Doctorate'] == 1))].reset_index()[['Title_New', 'Education_Tenth', 'Education_Twelvth', 'Education_Bachelors', 'Education_Masters', 'Education_Doctorate']]\n",
    "d['Title_New'] = d['Title_New'].astype('category')\n",
    "d['Categorical_Title'] = d['Title_New'].cat.codes\n",
    "d['Categorical_Title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Education_Tenth', 'Education_Twelvth', 'Education_Bachelors', 'Education_Masters', 'Education_Doctorate']\n",
    "cont_cols = []\n",
    "y_col = ['Categorical_Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = np.stack([d[col] for col in cat_cols], axis=1)\n",
    "cats = torch.tensor(cats, dtype=torch.int64)\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5340, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5340])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(d[y_col].values, dtype=torch.int64).flatten()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(90700, 50), (90700, 50), (90700, 50), (90700, 50), (90700, 50)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_szs = [len(df[col]) for col in cat_cols]\n",
    "emb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catz = cats[:4]\n",
    "catz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(90700, 50)\n",
       "  (1): Embedding(90700, 50)\n",
       "  (2): Embedding(90700, 50)\n",
       "  (3): Embedding(90700, 50)\n",
       "  (4): Embedding(90700, 50)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfembeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "selfembeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, Embedding(90700, 50)),\n",
       " (1, Embedding(90700, 50)),\n",
       " (2, Embedding(90700, 50)),\n",
       " (3, Embedding(90700, 50)),\n",
       " (4, Embedding(90700, 50))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(selfembeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.9117, -2.4320,  0.9120,  1.1426, -0.0574,  0.3116, -1.6541, -0.8689,\n",
       "           1.1779,  0.4037, -1.0449, -0.3239,  0.3059, -0.7542,  0.7342, -0.2572,\n",
       "           1.0755,  0.6817,  1.9141, -0.1283, -1.2220,  0.1389, -0.7962,  0.4024,\n",
       "          -0.4053, -1.5648,  1.2346,  0.3743, -1.2809,  0.1208, -1.1782,  1.0704,\n",
       "          -0.3191, -0.0422, -0.7438,  1.1159, -0.8702, -0.5665,  1.3211, -0.6472,\n",
       "          -1.0091, -2.3856,  0.1327, -0.0152,  1.4362,  2.5653,  1.1914,  0.4501,\n",
       "           2.1092, -0.4698],\n",
       "         [ 0.9117, -2.4320,  0.9120,  1.1426, -0.0574,  0.3116, -1.6541, -0.8689,\n",
       "           1.1779,  0.4037, -1.0449, -0.3239,  0.3059, -0.7542,  0.7342, -0.2572,\n",
       "           1.0755,  0.6817,  1.9141, -0.1283, -1.2220,  0.1389, -0.7962,  0.4024,\n",
       "          -0.4053, -1.5648,  1.2346,  0.3743, -1.2809,  0.1208, -1.1782,  1.0704,\n",
       "          -0.3191, -0.0422, -0.7438,  1.1159, -0.8702, -0.5665,  1.3211, -0.6472,\n",
       "          -1.0091, -2.3856,  0.1327, -0.0152,  1.4362,  2.5653,  1.1914,  0.4501,\n",
       "           2.1092, -0.4698],\n",
       "         [ 0.9117, -2.4320,  0.9120,  1.1426, -0.0574,  0.3116, -1.6541, -0.8689,\n",
       "           1.1779,  0.4037, -1.0449, -0.3239,  0.3059, -0.7542,  0.7342, -0.2572,\n",
       "           1.0755,  0.6817,  1.9141, -0.1283, -1.2220,  0.1389, -0.7962,  0.4024,\n",
       "          -0.4053, -1.5648,  1.2346,  0.3743, -1.2809,  0.1208, -1.1782,  1.0704,\n",
       "          -0.3191, -0.0422, -0.7438,  1.1159, -0.8702, -0.5665,  1.3211, -0.6472,\n",
       "          -1.0091, -2.3856,  0.1327, -0.0152,  1.4362,  2.5653,  1.1914,  0.4501,\n",
       "           2.1092, -0.4698],\n",
       "         [ 0.9117, -2.4320,  0.9120,  1.1426, -0.0574,  0.3116, -1.6541, -0.8689,\n",
       "           1.1779,  0.4037, -1.0449, -0.3239,  0.3059, -0.7542,  0.7342, -0.2572,\n",
       "           1.0755,  0.6817,  1.9141, -0.1283, -1.2220,  0.1389, -0.7962,  0.4024,\n",
       "          -0.4053, -1.5648,  1.2346,  0.3743, -1.2809,  0.1208, -1.1782,  1.0704,\n",
       "          -0.3191, -0.0422, -0.7438,  1.1159, -0.8702, -0.5665,  1.3211, -0.6472,\n",
       "          -1.0091, -2.3856,  0.1327, -0.0152,  1.4362,  2.5653,  1.1914,  0.4501,\n",
       "           2.1092, -0.4698]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[ 2.0129,  0.4562, -0.6227, -1.2657,  0.7417,  0.4831, -1.3611, -0.6021,\n",
       "          -0.0191, -1.7164, -0.4735,  1.0028,  0.5740,  1.3511, -1.1608,  0.7749,\n",
       "           0.4981,  0.5811, -1.1209,  0.4122,  0.9667,  0.3292,  0.6088, -1.6803,\n",
       "          -1.0636,  1.9971,  1.4741,  0.9539,  1.0946,  0.4552,  0.0856, -0.6064,\n",
       "           0.7899, -0.8798, -0.7902, -2.7178,  2.0392,  0.9844, -0.1817,  1.0628,\n",
       "          -1.7164, -0.3907,  0.5818,  1.6086, -0.6816, -0.1642,  1.7006,  0.0863,\n",
       "          -1.2976, -0.4570],\n",
       "         [-1.2090, -0.6782,  0.3786,  0.3899,  0.4351,  0.3333,  1.3257, -1.6073,\n",
       "          -0.2747,  0.1870,  0.5462,  0.3114,  1.0071, -1.0306,  0.6779,  0.3777,\n",
       "           0.7304, -1.1599,  0.5697, -1.8373, -0.3279,  0.0206,  1.3203,  0.6551,\n",
       "           1.4186,  0.1326, -0.0741,  0.9775, -2.0661,  0.4806,  0.5665,  1.4114,\n",
       "          -0.2785,  0.2326, -0.2189,  0.0360, -0.6893,  1.2075,  1.0128,  1.0324,\n",
       "           0.0718, -1.1106, -1.4590,  1.2498,  0.2877,  0.3895,  0.3033, -0.2410,\n",
       "          -0.8091,  1.2649],\n",
       "         [ 2.0129,  0.4562, -0.6227, -1.2657,  0.7417,  0.4831, -1.3611, -0.6021,\n",
       "          -0.0191, -1.7164, -0.4735,  1.0028,  0.5740,  1.3511, -1.1608,  0.7749,\n",
       "           0.4981,  0.5811, -1.1209,  0.4122,  0.9667,  0.3292,  0.6088, -1.6803,\n",
       "          -1.0636,  1.9971,  1.4741,  0.9539,  1.0946,  0.4552,  0.0856, -0.6064,\n",
       "           0.7899, -0.8798, -0.7902, -2.7178,  2.0392,  0.9844, -0.1817,  1.0628,\n",
       "          -1.7164, -0.3907,  0.5818,  1.6086, -0.6816, -0.1642,  1.7006,  0.0863,\n",
       "          -1.2976, -0.4570],\n",
       "         [ 2.0129,  0.4562, -0.6227, -1.2657,  0.7417,  0.4831, -1.3611, -0.6021,\n",
       "          -0.0191, -1.7164, -0.4735,  1.0028,  0.5740,  1.3511, -1.1608,  0.7749,\n",
       "           0.4981,  0.5811, -1.1209,  0.4122,  0.9667,  0.3292,  0.6088, -1.6803,\n",
       "          -1.0636,  1.9971,  1.4741,  0.9539,  1.0946,  0.4552,  0.0856, -0.6064,\n",
       "           0.7899, -0.8798, -0.7902, -2.7178,  2.0392,  0.9844, -0.1817,  1.0628,\n",
       "          -1.7164, -0.3907,  0.5818,  1.6086, -0.6816, -0.1642,  1.7006,  0.0863,\n",
       "          -1.2976, -0.4570]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[-0.1855, -0.1479,  0.7251, -1.0226, -0.5907, -0.8027,  0.7557, -0.2695,\n",
       "          -0.3875,  1.4570,  0.9226, -2.0241,  0.5200,  0.5536, -0.3264,  0.2654,\n",
       "          -0.7557,  0.0332, -0.1582,  2.4620, -1.0823, -0.9076,  0.3803,  0.0266,\n",
       "          -0.5460,  1.5889, -0.5041,  0.1861,  0.9483,  0.9490,  0.9119, -0.9559,\n",
       "          -1.4257,  0.5471,  0.2868, -0.6635,  0.7831, -0.3637, -1.1330,  0.0260,\n",
       "          -0.3578,  0.7700,  0.1575,  0.4403,  0.5386, -0.5117,  0.2013,  0.8023,\n",
       "           0.6044, -0.6868],\n",
       "         [-0.1855, -0.1479,  0.7251, -1.0226, -0.5907, -0.8027,  0.7557, -0.2695,\n",
       "          -0.3875,  1.4570,  0.9226, -2.0241,  0.5200,  0.5536, -0.3264,  0.2654,\n",
       "          -0.7557,  0.0332, -0.1582,  2.4620, -1.0823, -0.9076,  0.3803,  0.0266,\n",
       "          -0.5460,  1.5889, -0.5041,  0.1861,  0.9483,  0.9490,  0.9119, -0.9559,\n",
       "          -1.4257,  0.5471,  0.2868, -0.6635,  0.7831, -0.3637, -1.1330,  0.0260,\n",
       "          -0.3578,  0.7700,  0.1575,  0.4403,  0.5386, -0.5117,  0.2013,  0.8023,\n",
       "           0.6044, -0.6868],\n",
       "         [-0.1855, -0.1479,  0.7251, -1.0226, -0.5907, -0.8027,  0.7557, -0.2695,\n",
       "          -0.3875,  1.4570,  0.9226, -2.0241,  0.5200,  0.5536, -0.3264,  0.2654,\n",
       "          -0.7557,  0.0332, -0.1582,  2.4620, -1.0823, -0.9076,  0.3803,  0.0266,\n",
       "          -0.5460,  1.5889, -0.5041,  0.1861,  0.9483,  0.9490,  0.9119, -0.9559,\n",
       "          -1.4257,  0.5471,  0.2868, -0.6635,  0.7831, -0.3637, -1.1330,  0.0260,\n",
       "          -0.3578,  0.7700,  0.1575,  0.4403,  0.5386, -0.5117,  0.2013,  0.8023,\n",
       "           0.6044, -0.6868],\n",
       "         [-0.1855, -0.1479,  0.7251, -1.0226, -0.5907, -0.8027,  0.7557, -0.2695,\n",
       "          -0.3875,  1.4570,  0.9226, -2.0241,  0.5200,  0.5536, -0.3264,  0.2654,\n",
       "          -0.7557,  0.0332, -0.1582,  2.4620, -1.0823, -0.9076,  0.3803,  0.0266,\n",
       "          -0.5460,  1.5889, -0.5041,  0.1861,  0.9483,  0.9490,  0.9119, -0.9559,\n",
       "          -1.4257,  0.5471,  0.2868, -0.6635,  0.7831, -0.3637, -1.1330,  0.0260,\n",
       "          -0.3578,  0.7700,  0.1575,  0.4403,  0.5386, -0.5117,  0.2013,  0.8023,\n",
       "           0.6044, -0.6868]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[ 0.1756,  0.9037,  0.6179,  0.8463, -0.2128,  1.7576, -1.4068, -0.4717,\n",
       "           0.7340, -0.9084,  0.3633, -0.1573, -2.0968, -1.0427,  1.1905,  0.5305,\n",
       "          -0.7071,  0.4008,  0.6301,  0.2546, -0.1700,  0.6565, -0.1738,  0.2700,\n",
       "          -0.1713,  1.1189,  0.5054,  0.6316, -1.0177,  0.4913,  0.6912, -0.9077,\n",
       "          -0.5163,  0.2799,  0.5458, -0.1719,  0.8843,  0.1459,  1.1354,  0.3098,\n",
       "           0.3801, -0.1528,  2.0557,  0.4977, -0.6244,  0.7658, -0.7381, -0.8304,\n",
       "          -0.5793, -1.5571],\n",
       "         [ 0.1756,  0.9037,  0.6179,  0.8463, -0.2128,  1.7576, -1.4068, -0.4717,\n",
       "           0.7340, -0.9084,  0.3633, -0.1573, -2.0968, -1.0427,  1.1905,  0.5305,\n",
       "          -0.7071,  0.4008,  0.6301,  0.2546, -0.1700,  0.6565, -0.1738,  0.2700,\n",
       "          -0.1713,  1.1189,  0.5054,  0.6316, -1.0177,  0.4913,  0.6912, -0.9077,\n",
       "          -0.5163,  0.2799,  0.5458, -0.1719,  0.8843,  0.1459,  1.1354,  0.3098,\n",
       "           0.3801, -0.1528,  2.0557,  0.4977, -0.6244,  0.7658, -0.7381, -0.8304,\n",
       "          -0.5793, -1.5571],\n",
       "         [ 0.1756,  0.9037,  0.6179,  0.8463, -0.2128,  1.7576, -1.4068, -0.4717,\n",
       "           0.7340, -0.9084,  0.3633, -0.1573, -2.0968, -1.0427,  1.1905,  0.5305,\n",
       "          -0.7071,  0.4008,  0.6301,  0.2546, -0.1700,  0.6565, -0.1738,  0.2700,\n",
       "          -0.1713,  1.1189,  0.5054,  0.6316, -1.0177,  0.4913,  0.6912, -0.9077,\n",
       "          -0.5163,  0.2799,  0.5458, -0.1719,  0.8843,  0.1459,  1.1354,  0.3098,\n",
       "           0.3801, -0.1528,  2.0557,  0.4977, -0.6244,  0.7658, -0.7381, -0.8304,\n",
       "          -0.5793, -1.5571],\n",
       "         [ 0.1756,  0.9037,  0.6179,  0.8463, -0.2128,  1.7576, -1.4068, -0.4717,\n",
       "           0.7340, -0.9084,  0.3633, -0.1573, -2.0968, -1.0427,  1.1905,  0.5305,\n",
       "          -0.7071,  0.4008,  0.6301,  0.2546, -0.1700,  0.6565, -0.1738,  0.2700,\n",
       "          -0.1713,  1.1189,  0.5054,  0.6316, -1.0177,  0.4913,  0.6912, -0.9077,\n",
       "          -0.5163,  0.2799,  0.5458, -0.1719,  0.8843,  0.1459,  1.1354,  0.3098,\n",
       "           0.3801, -0.1528,  2.0557,  0.4977, -0.6244,  0.7658, -0.7381, -0.8304,\n",
       "          -0.5793, -1.5571]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[ 1.3839, -2.8244, -0.4470, -1.7701, -2.5935,  0.6855,  0.4364,  0.6662,\n",
       "           0.7951,  0.8990,  0.0751,  0.0147, -0.0106, -1.6197, -0.0369, -0.4128,\n",
       "          -1.2866, -1.9463,  0.4870,  0.8172, -0.7278, -0.2902, -0.5373,  1.5994,\n",
       "          -0.4669,  0.5382,  0.4654, -1.1866, -1.3213,  0.3199, -1.2631, -0.3669,\n",
       "          -0.5040,  0.0185,  0.8627, -0.8261,  0.6384,  1.1309,  0.4376,  1.1969,\n",
       "          -1.1574, -1.0738, -1.4048, -1.0614, -0.0031,  0.6926,  1.1977,  0.6722,\n",
       "          -0.6319, -0.3247],\n",
       "         [ 1.3839, -2.8244, -0.4470, -1.7701, -2.5935,  0.6855,  0.4364,  0.6662,\n",
       "           0.7951,  0.8990,  0.0751,  0.0147, -0.0106, -1.6197, -0.0369, -0.4128,\n",
       "          -1.2866, -1.9463,  0.4870,  0.8172, -0.7278, -0.2902, -0.5373,  1.5994,\n",
       "          -0.4669,  0.5382,  0.4654, -1.1866, -1.3213,  0.3199, -1.2631, -0.3669,\n",
       "          -0.5040,  0.0185,  0.8627, -0.8261,  0.6384,  1.1309,  0.4376,  1.1969,\n",
       "          -1.1574, -1.0738, -1.4048, -1.0614, -0.0031,  0.6926,  1.1977,  0.6722,\n",
       "          -0.6319, -0.3247],\n",
       "         [ 1.3839, -2.8244, -0.4470, -1.7701, -2.5935,  0.6855,  0.4364,  0.6662,\n",
       "           0.7951,  0.8990,  0.0751,  0.0147, -0.0106, -1.6197, -0.0369, -0.4128,\n",
       "          -1.2866, -1.9463,  0.4870,  0.8172, -0.7278, -0.2902, -0.5373,  1.5994,\n",
       "          -0.4669,  0.5382,  0.4654, -1.1866, -1.3213,  0.3199, -1.2631, -0.3669,\n",
       "          -0.5040,  0.0185,  0.8627, -0.8261,  0.6384,  1.1309,  0.4376,  1.1969,\n",
       "          -1.1574, -1.0738, -1.4048, -1.0614, -0.0031,  0.6926,  1.1977,  0.6722,\n",
       "          -0.6319, -0.3247],\n",
       "         [ 1.3839, -2.8244, -0.4470, -1.7701, -2.5935,  0.6855,  0.4364,  0.6662,\n",
       "           0.7951,  0.8990,  0.0751,  0.0147, -0.0106, -1.6197, -0.0369, -0.4128,\n",
       "          -1.2866, -1.9463,  0.4870,  0.8172, -0.7278, -0.2902, -0.5373,  1.5994,\n",
       "          -0.4669,  0.5382,  0.4654, -1.1866, -1.3213,  0.3199, -1.2631, -0.3669,\n",
       "          -0.5040,  0.0185,  0.8627, -0.8261,  0.6384,  1.1309,  0.4376,  1.1969,\n",
       "          -1.1574, -1.0738, -1.4048, -1.0614, -0.0031,  0.6926,  1.1977,  0.6722,\n",
       "          -0.6319, -0.3247]], grad_fn=<EmbeddingBackward>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingz = []\n",
    "for i,e in enumerate(selfembeds):\n",
    "    embeddingz.append(e(catz[:,i]))\n",
    "embeddingz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9117, -2.4320,  0.9120,  1.1426, -0.0574,  0.3116, -1.6541, -0.8689,\n",
       "          1.1779,  0.4037, -1.0449, -0.3239,  0.3059, -0.7542,  0.7342, -0.2572,\n",
       "          1.0755,  0.6817,  1.9141, -0.1283, -1.2220,  0.1389, -0.7962,  0.4024,\n",
       "         -0.4053, -1.5648,  1.2346,  0.3743, -1.2809,  0.1208, -1.1782,  1.0704,\n",
       "         -0.3191, -0.0422, -0.7438,  1.1159, -0.8702, -0.5665,  1.3211, -0.6472,\n",
       "         -1.0091, -2.3856,  0.1327, -0.0152,  1.4362,  2.5653,  1.1914,  0.4501,\n",
       "          2.1092, -0.4698,  2.0129,  0.4562, -0.6227, -1.2657,  0.7417,  0.4831,\n",
       "         -1.3611, -0.6021, -0.0191, -1.7164, -0.4735,  1.0028,  0.5740,  1.3511,\n",
       "         -1.1608,  0.7749,  0.4981,  0.5811, -1.1209,  0.4122,  0.9667,  0.3292,\n",
       "          0.6088, -1.6803, -1.0636,  1.9971,  1.4741,  0.9539,  1.0946,  0.4552,\n",
       "          0.0856, -0.6064,  0.7899, -0.8798, -0.7902, -2.7178,  2.0392,  0.9844,\n",
       "         -0.1817,  1.0628, -1.7164, -0.3907,  0.5818,  1.6086, -0.6816, -0.1642,\n",
       "          1.7006,  0.0863, -1.2976, -0.4570, -0.1855, -0.1479,  0.7251, -1.0226,\n",
       "         -0.5907, -0.8027,  0.7557, -0.2695, -0.3875,  1.4570,  0.9226, -2.0241,\n",
       "          0.5200,  0.5536, -0.3264,  0.2654, -0.7557,  0.0332, -0.1582,  2.4620,\n",
       "         -1.0823, -0.9076,  0.3803,  0.0266, -0.5460,  1.5889, -0.5041,  0.1861,\n",
       "          0.9483,  0.9490,  0.9119, -0.9559, -1.4257,  0.5471,  0.2868, -0.6635,\n",
       "          0.7831, -0.3637, -1.1330,  0.0260, -0.3578,  0.7700,  0.1575,  0.4403,\n",
       "          0.5386, -0.5117,  0.2013,  0.8023,  0.6044, -0.6868,  0.1756,  0.9037,\n",
       "          0.6179,  0.8463, -0.2128,  1.7576, -1.4068, -0.4717,  0.7340, -0.9084,\n",
       "          0.3633, -0.1573, -2.0968, -1.0427,  1.1905,  0.5305, -0.7071,  0.4008,\n",
       "          0.6301,  0.2546, -0.1700,  0.6565, -0.1738,  0.2700, -0.1713,  1.1189,\n",
       "          0.5054,  0.6316, -1.0177,  0.4913,  0.6912, -0.9077, -0.5163,  0.2799,\n",
       "          0.5458, -0.1719,  0.8843,  0.1459,  1.1354,  0.3098,  0.3801, -0.1528,\n",
       "          2.0557,  0.4977, -0.6244,  0.7658, -0.7381, -0.8304, -0.5793, -1.5571,\n",
       "          1.3839, -2.8244, -0.4470, -1.7701, -2.5935,  0.6855,  0.4364,  0.6662,\n",
       "          0.7951,  0.8990,  0.0751,  0.0147, -0.0106, -1.6197, -0.0369, -0.4128,\n",
       "         -1.2866, -1.9463,  0.4870,  0.8172, -0.7278, -0.2902, -0.5373,  1.5994,\n",
       "         -0.4669,  0.5382,  0.4654, -1.1866, -1.3213,  0.3199, -1.2631, -0.3669,\n",
       "         -0.5040,  0.0185,  0.8627, -0.8261,  0.6384,  1.1309,  0.4376,  1.1969,\n",
       "         -1.1574, -1.0738, -1.4048, -1.0614, -0.0031,  0.6926,  1.1977,  0.6722,\n",
       "         -0.6319, -0.3247],\n",
       "        [ 0.9117, -2.4320,  0.9120,  1.1426, -0.0574,  0.3116, -1.6541, -0.8689,\n",
       "          1.1779,  0.4037, -1.0449, -0.3239,  0.3059, -0.7542,  0.7342, -0.2572,\n",
       "          1.0755,  0.6817,  1.9141, -0.1283, -1.2220,  0.1389, -0.7962,  0.4024,\n",
       "         -0.4053, -1.5648,  1.2346,  0.3743, -1.2809,  0.1208, -1.1782,  1.0704,\n",
       "         -0.3191, -0.0422, -0.7438,  1.1159, -0.8702, -0.5665,  1.3211, -0.6472,\n",
       "         -1.0091, -2.3856,  0.1327, -0.0152,  1.4362,  2.5653,  1.1914,  0.4501,\n",
       "          2.1092, -0.4698, -1.2090, -0.6782,  0.3786,  0.3899,  0.4351,  0.3333,\n",
       "          1.3257, -1.6073, -0.2747,  0.1870,  0.5462,  0.3114,  1.0071, -1.0306,\n",
       "          0.6779,  0.3777,  0.7304, -1.1599,  0.5697, -1.8373, -0.3279,  0.0206,\n",
       "          1.3203,  0.6551,  1.4186,  0.1326, -0.0741,  0.9775, -2.0661,  0.4806,\n",
       "          0.5665,  1.4114, -0.2785,  0.2326, -0.2189,  0.0360, -0.6893,  1.2075,\n",
       "          1.0128,  1.0324,  0.0718, -1.1106, -1.4590,  1.2498,  0.2877,  0.3895,\n",
       "          0.3033, -0.2410, -0.8091,  1.2649, -0.1855, -0.1479,  0.7251, -1.0226,\n",
       "         -0.5907, -0.8027,  0.7557, -0.2695, -0.3875,  1.4570,  0.9226, -2.0241,\n",
       "          0.5200,  0.5536, -0.3264,  0.2654, -0.7557,  0.0332, -0.1582,  2.4620,\n",
       "         -1.0823, -0.9076,  0.3803,  0.0266, -0.5460,  1.5889, -0.5041,  0.1861,\n",
       "          0.9483,  0.9490,  0.9119, -0.9559, -1.4257,  0.5471,  0.2868, -0.6635,\n",
       "          0.7831, -0.3637, -1.1330,  0.0260, -0.3578,  0.7700,  0.1575,  0.4403,\n",
       "          0.5386, -0.5117,  0.2013,  0.8023,  0.6044, -0.6868,  0.1756,  0.9037,\n",
       "          0.6179,  0.8463, -0.2128,  1.7576, -1.4068, -0.4717,  0.7340, -0.9084,\n",
       "          0.3633, -0.1573, -2.0968, -1.0427,  1.1905,  0.5305, -0.7071,  0.4008,\n",
       "          0.6301,  0.2546, -0.1700,  0.6565, -0.1738,  0.2700, -0.1713,  1.1189,\n",
       "          0.5054,  0.6316, -1.0177,  0.4913,  0.6912, -0.9077, -0.5163,  0.2799,\n",
       "          0.5458, -0.1719,  0.8843,  0.1459,  1.1354,  0.3098,  0.3801, -0.1528,\n",
       "          2.0557,  0.4977, -0.6244,  0.7658, -0.7381, -0.8304, -0.5793, -1.5571,\n",
       "          1.3839, -2.8244, -0.4470, -1.7701, -2.5935,  0.6855,  0.4364,  0.6662,\n",
       "          0.7951,  0.8990,  0.0751,  0.0147, -0.0106, -1.6197, -0.0369, -0.4128,\n",
       "         -1.2866, -1.9463,  0.4870,  0.8172, -0.7278, -0.2902, -0.5373,  1.5994,\n",
       "         -0.4669,  0.5382,  0.4654, -1.1866, -1.3213,  0.3199, -1.2631, -0.3669,\n",
       "         -0.5040,  0.0185,  0.8627, -0.8261,  0.6384,  1.1309,  0.4376,  1.1969,\n",
       "         -1.1574, -1.0738, -1.4048, -1.0614, -0.0031,  0.6926,  1.1977,  0.6722,\n",
       "         -0.6319, -0.3247],\n",
       "        [ 0.9117, -2.4320,  0.9120,  1.1426, -0.0574,  0.3116, -1.6541, -0.8689,\n",
       "          1.1779,  0.4037, -1.0449, -0.3239,  0.3059, -0.7542,  0.7342, -0.2572,\n",
       "          1.0755,  0.6817,  1.9141, -0.1283, -1.2220,  0.1389, -0.7962,  0.4024,\n",
       "         -0.4053, -1.5648,  1.2346,  0.3743, -1.2809,  0.1208, -1.1782,  1.0704,\n",
       "         -0.3191, -0.0422, -0.7438,  1.1159, -0.8702, -0.5665,  1.3211, -0.6472,\n",
       "         -1.0091, -2.3856,  0.1327, -0.0152,  1.4362,  2.5653,  1.1914,  0.4501,\n",
       "          2.1092, -0.4698,  2.0129,  0.4562, -0.6227, -1.2657,  0.7417,  0.4831,\n",
       "         -1.3611, -0.6021, -0.0191, -1.7164, -0.4735,  1.0028,  0.5740,  1.3511,\n",
       "         -1.1608,  0.7749,  0.4981,  0.5811, -1.1209,  0.4122,  0.9667,  0.3292,\n",
       "          0.6088, -1.6803, -1.0636,  1.9971,  1.4741,  0.9539,  1.0946,  0.4552,\n",
       "          0.0856, -0.6064,  0.7899, -0.8798, -0.7902, -2.7178,  2.0392,  0.9844,\n",
       "         -0.1817,  1.0628, -1.7164, -0.3907,  0.5818,  1.6086, -0.6816, -0.1642,\n",
       "          1.7006,  0.0863, -1.2976, -0.4570, -0.1855, -0.1479,  0.7251, -1.0226,\n",
       "         -0.5907, -0.8027,  0.7557, -0.2695, -0.3875,  1.4570,  0.9226, -2.0241,\n",
       "          0.5200,  0.5536, -0.3264,  0.2654, -0.7557,  0.0332, -0.1582,  2.4620,\n",
       "         -1.0823, -0.9076,  0.3803,  0.0266, -0.5460,  1.5889, -0.5041,  0.1861,\n",
       "          0.9483,  0.9490,  0.9119, -0.9559, -1.4257,  0.5471,  0.2868, -0.6635,\n",
       "          0.7831, -0.3637, -1.1330,  0.0260, -0.3578,  0.7700,  0.1575,  0.4403,\n",
       "          0.5386, -0.5117,  0.2013,  0.8023,  0.6044, -0.6868,  0.1756,  0.9037,\n",
       "          0.6179,  0.8463, -0.2128,  1.7576, -1.4068, -0.4717,  0.7340, -0.9084,\n",
       "          0.3633, -0.1573, -2.0968, -1.0427,  1.1905,  0.5305, -0.7071,  0.4008,\n",
       "          0.6301,  0.2546, -0.1700,  0.6565, -0.1738,  0.2700, -0.1713,  1.1189,\n",
       "          0.5054,  0.6316, -1.0177,  0.4913,  0.6912, -0.9077, -0.5163,  0.2799,\n",
       "          0.5458, -0.1719,  0.8843,  0.1459,  1.1354,  0.3098,  0.3801, -0.1528,\n",
       "          2.0557,  0.4977, -0.6244,  0.7658, -0.7381, -0.8304, -0.5793, -1.5571,\n",
       "          1.3839, -2.8244, -0.4470, -1.7701, -2.5935,  0.6855,  0.4364,  0.6662,\n",
       "          0.7951,  0.8990,  0.0751,  0.0147, -0.0106, -1.6197, -0.0369, -0.4128,\n",
       "         -1.2866, -1.9463,  0.4870,  0.8172, -0.7278, -0.2902, -0.5373,  1.5994,\n",
       "         -0.4669,  0.5382,  0.4654, -1.1866, -1.3213,  0.3199, -1.2631, -0.3669,\n",
       "         -0.5040,  0.0185,  0.8627, -0.8261,  0.6384,  1.1309,  0.4376,  1.1969,\n",
       "         -1.1574, -1.0738, -1.4048, -1.0614, -0.0031,  0.6926,  1.1977,  0.6722,\n",
       "         -0.6319, -0.3247],\n",
       "        [ 0.9117, -2.4320,  0.9120,  1.1426, -0.0574,  0.3116, -1.6541, -0.8689,\n",
       "          1.1779,  0.4037, -1.0449, -0.3239,  0.3059, -0.7542,  0.7342, -0.2572,\n",
       "          1.0755,  0.6817,  1.9141, -0.1283, -1.2220,  0.1389, -0.7962,  0.4024,\n",
       "         -0.4053, -1.5648,  1.2346,  0.3743, -1.2809,  0.1208, -1.1782,  1.0704,\n",
       "         -0.3191, -0.0422, -0.7438,  1.1159, -0.8702, -0.5665,  1.3211, -0.6472,\n",
       "         -1.0091, -2.3856,  0.1327, -0.0152,  1.4362,  2.5653,  1.1914,  0.4501,\n",
       "          2.1092, -0.4698,  2.0129,  0.4562, -0.6227, -1.2657,  0.7417,  0.4831,\n",
       "         -1.3611, -0.6021, -0.0191, -1.7164, -0.4735,  1.0028,  0.5740,  1.3511,\n",
       "         -1.1608,  0.7749,  0.4981,  0.5811, -1.1209,  0.4122,  0.9667,  0.3292,\n",
       "          0.6088, -1.6803, -1.0636,  1.9971,  1.4741,  0.9539,  1.0946,  0.4552,\n",
       "          0.0856, -0.6064,  0.7899, -0.8798, -0.7902, -2.7178,  2.0392,  0.9844,\n",
       "         -0.1817,  1.0628, -1.7164, -0.3907,  0.5818,  1.6086, -0.6816, -0.1642,\n",
       "          1.7006,  0.0863, -1.2976, -0.4570, -0.1855, -0.1479,  0.7251, -1.0226,\n",
       "         -0.5907, -0.8027,  0.7557, -0.2695, -0.3875,  1.4570,  0.9226, -2.0241,\n",
       "          0.5200,  0.5536, -0.3264,  0.2654, -0.7557,  0.0332, -0.1582,  2.4620,\n",
       "         -1.0823, -0.9076,  0.3803,  0.0266, -0.5460,  1.5889, -0.5041,  0.1861,\n",
       "          0.9483,  0.9490,  0.9119, -0.9559, -1.4257,  0.5471,  0.2868, -0.6635,\n",
       "          0.7831, -0.3637, -1.1330,  0.0260, -0.3578,  0.7700,  0.1575,  0.4403,\n",
       "          0.5386, -0.5117,  0.2013,  0.8023,  0.6044, -0.6868,  0.1756,  0.9037,\n",
       "          0.6179,  0.8463, -0.2128,  1.7576, -1.4068, -0.4717,  0.7340, -0.9084,\n",
       "          0.3633, -0.1573, -2.0968, -1.0427,  1.1905,  0.5305, -0.7071,  0.4008,\n",
       "          0.6301,  0.2546, -0.1700,  0.6565, -0.1738,  0.2700, -0.1713,  1.1189,\n",
       "          0.5054,  0.6316, -1.0177,  0.4913,  0.6912, -0.9077, -0.5163,  0.2799,\n",
       "          0.5458, -0.1719,  0.8843,  0.1459,  1.1354,  0.3098,  0.3801, -0.1528,\n",
       "          2.0557,  0.4977, -0.6244,  0.7658, -0.7381, -0.8304, -0.5793, -1.5571,\n",
       "          1.3839, -2.8244, -0.4470, -1.7701, -2.5935,  0.6855,  0.4364,  0.6662,\n",
       "          0.7951,  0.8990,  0.0751,  0.0147, -0.0106, -1.6197, -0.0369, -0.4128,\n",
       "         -1.2866, -1.9463,  0.4870,  0.8172, -0.7278, -0.2902, -0.5373,  1.5994,\n",
       "         -0.4669,  0.5382,  0.4654, -1.1866, -1.3213,  0.3199, -1.2631, -0.3669,\n",
       "         -0.5040,  0.0185,  0.8627, -0.8261,  0.6384,  1.1309,  0.4376,  1.1969,\n",
       "         -1.1574, -1.0738, -1.4048, -1.0614, -0.0031,  0.6926,  1.1977,  0.6722,\n",
       "         -0.6319, -0.3247]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.cat(embeddingz, 1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfembdrop = nn.Dropout(.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -4.0533,  1.5200,  0.0000, -0.0000,  0.5193, -0.0000, -1.4482,\n",
       "          0.0000,  0.0000, -0.0000, -0.5399,  0.5098, -1.2570,  1.2236, -0.4287,\n",
       "          1.7924,  0.0000,  3.1902, -0.2139, -2.0366,  0.0000, -0.0000,  0.6706,\n",
       "         -0.6754, -2.6080,  2.0576,  0.6239, -0.0000,  0.2013, -0.0000,  1.7841,\n",
       "         -0.5318, -0.0000, -1.2397,  1.8599, -0.0000, -0.9442,  0.0000, -1.0786,\n",
       "         -1.6819, -3.9760,  0.0000, -0.0253,  0.0000,  0.0000,  1.9857,  0.7502,\n",
       "          0.0000, -0.7830,  3.3548,  0.7604, -0.0000, -0.0000,  0.0000,  0.8052,\n",
       "         -2.2686, -0.0000, -0.0319, -2.8607, -0.0000,  1.6714,  0.0000,  0.0000,\n",
       "         -1.9346,  1.2915,  0.0000,  0.0000, -1.8682,  0.6871,  0.0000,  0.5487,\n",
       "          1.0147, -0.0000, -1.7727,  3.3285,  2.4568,  1.5898,  1.8243,  0.7587,\n",
       "          0.0000, -0.0000,  0.0000, -1.4664, -0.0000, -0.0000,  3.3987,  1.6406,\n",
       "         -0.3028,  0.0000, -2.8606, -0.0000,  0.9696,  0.0000, -0.0000, -0.2736,\n",
       "          0.0000,  0.0000, -2.1627, -0.7617, -0.0000, -0.0000,  0.0000, -1.7044,\n",
       "         -0.0000, -0.0000,  1.2594, -0.4492, -0.6458,  2.4284,  0.0000, -0.0000,\n",
       "          0.0000,  0.9227, -0.5441,  0.4423, -0.0000,  0.0000, -0.2636,  4.1033,\n",
       "         -1.8038, -0.0000,  0.0000,  0.0444, -0.0000,  2.6481, -0.8402,  0.0000,\n",
       "          1.5805,  0.0000,  0.0000, -0.0000, -2.3762,  0.9118,  0.4780, -1.1059,\n",
       "          1.3051, -0.6062, -1.8884,  0.0433, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.8529,  0.0000,  0.0000,  0.0000, -1.1446,  0.2926,  1.5061,\n",
       "          1.0299,  1.4105, -0.3547,  0.0000, -2.3446, -0.7861,  1.2233, -1.5140,\n",
       "          0.6055, -0.2621, -3.4947, -1.7378,  1.9842,  0.0000, -1.1785,  0.6680,\n",
       "          1.0502,  0.4244, -0.2834,  0.0000, -0.0000,  0.4500, -0.0000,  1.8649,\n",
       "          0.0000,  1.0526, -0.0000,  0.8188,  1.1520, -0.0000, -0.8605,  0.4665,\n",
       "          0.0000, -0.2865,  1.4739,  0.0000,  1.8924,  0.5163,  0.6334, -0.0000,\n",
       "          3.4261,  0.8295, -1.0407,  1.2763, -1.2302, -0.0000, -0.9655, -2.5952,\n",
       "          2.3066, -4.7073, -0.7449, -2.9501, -4.3226,  0.0000,  0.7273,  1.1103,\n",
       "          1.3251,  0.0000,  0.1252,  0.0245, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "         -2.1444, -3.2439,  0.0000,  1.3619, -0.0000, -0.0000, -0.0000,  2.6656,\n",
       "         -0.7782,  0.8970,  0.0000, -1.9777, -2.2021,  0.0000, -2.1052, -0.0000,\n",
       "         -0.8401,  0.0309,  0.0000, -1.3769,  1.0640,  1.8849,  0.7293,  0.0000,\n",
       "         -1.9289, -0.0000, -2.3413, -1.7690, -0.0000,  0.0000,  0.0000,  1.1203,\n",
       "         -1.0532, -0.0000],\n",
       "        [ 1.5195, -4.0533,  1.5200,  1.9044, -0.0956,  0.5193, -2.7568, -1.4482,\n",
       "          0.0000,  0.6729, -0.0000, -0.5399,  0.5098, -1.2570,  1.2236, -0.0000,\n",
       "          1.7924,  0.0000,  3.1902, -0.0000, -2.0366,  0.2314, -0.0000,  0.6706,\n",
       "         -0.6754, -2.6080,  2.0576,  0.6239, -0.0000,  0.2013, -0.0000,  0.0000,\n",
       "         -0.0000, -0.0000, -1.2397,  1.8599, -1.4503, -0.9442,  2.2018, -0.0000,\n",
       "         -1.6819, -3.9760,  0.2212, -0.0253,  2.3937,  4.2756,  1.9857,  0.7502,\n",
       "          3.5154, -0.0000, -2.0150, -1.1304,  0.0000,  0.6498,  0.7251,  0.5555,\n",
       "          0.0000, -2.6788, -0.4579,  0.3116,  0.9103,  0.5189,  0.0000, -0.0000,\n",
       "          0.0000,  0.6295,  0.0000, -1.9332,  0.9494, -0.0000, -0.5465,  0.0344,\n",
       "          2.2005,  0.0000,  2.3643,  0.0000, -0.1235,  1.6292, -3.4435,  0.0000,\n",
       "          0.0000,  0.0000, -0.4642,  0.0000, -0.0000,  0.0600, -1.1489,  0.0000,\n",
       "          0.0000,  0.0000,  0.1196, -0.0000, -0.0000,  2.0831,  0.4795,  0.0000,\n",
       "          0.5054, -0.0000, -0.0000,  2.1082, -0.3092, -0.0000,  0.0000, -1.7044,\n",
       "         -0.9845, -0.0000,  1.2594, -0.4492, -0.0000,  0.0000,  1.5376, -0.0000,\n",
       "          0.8667,  0.9227, -0.5441,  0.4423, -0.0000,  0.0000, -0.0000,  4.1033,\n",
       "         -1.8038, -0.0000,  0.6338,  0.0444, -0.0000,  2.6481, -0.8402,  0.0000,\n",
       "          1.5805,  1.5817,  1.5198, -1.5931, -0.0000,  0.9118,  0.0000, -0.0000,\n",
       "          1.3051, -0.6062, -0.0000,  0.0433, -0.5964,  1.2833,  0.2625,  0.7339,\n",
       "          0.8977, -0.0000,  0.0000,  1.3371,  1.0074, -0.0000,  0.2926,  0.0000,\n",
       "          1.0299,  1.4105, -0.0000,  2.9294, -0.0000, -0.0000,  0.0000, -0.0000,\n",
       "          0.0000, -0.0000, -0.0000, -0.0000,  1.9842,  0.8842, -1.1785,  0.0000,\n",
       "          1.0502,  0.4244, -0.0000,  0.0000, -0.0000,  0.4500, -0.0000,  0.0000,\n",
       "          0.8423,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.8605,  0.4665,\n",
       "          0.0000, -0.2865,  1.4739,  0.2432,  1.8924,  0.5163,  0.6334, -0.2547,\n",
       "          0.0000,  0.8295, -0.0000,  1.2763, -0.0000, -0.0000, -0.0000, -2.5952,\n",
       "          2.3066, -0.0000, -0.7449, -2.9501, -4.3226,  0.0000,  0.7273,  1.1103,\n",
       "          1.3251,  1.4984,  0.0000,  0.0245, -0.0177, -2.6994, -0.0615, -0.0000,\n",
       "         -2.1444, -0.0000,  0.0000,  0.0000, -1.2131, -0.4837, -0.0000,  0.0000,\n",
       "         -0.7782,  0.8970,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
       "         -0.8401,  0.0000,  0.0000, -0.0000,  1.0640,  1.8849,  0.7293,  1.9949,\n",
       "         -1.9289, -1.7897, -2.3413, -1.7690, -0.0051,  0.0000,  0.0000,  0.0000,\n",
       "         -1.0532, -0.0000],\n",
       "        [ 1.5195, -0.0000,  1.5200,  0.0000, -0.0956,  0.5193, -0.0000, -0.0000,\n",
       "          1.9632,  0.6729, -1.7414, -0.0000,  0.5098, -1.2570,  0.0000, -0.4287,\n",
       "          1.7924,  1.1361,  3.1902, -0.2139, -2.0366,  0.0000, -1.3270,  0.6706,\n",
       "         -0.6754, -0.0000,  2.0576,  0.6239, -0.0000,  0.0000, -1.9637,  0.0000,\n",
       "         -0.5318, -0.0703, -1.2397,  1.8599, -1.4503, -0.9442,  2.2018, -1.0786,\n",
       "         -1.6819, -0.0000,  0.2212, -0.0253,  0.0000,  0.0000,  0.0000,  0.7502,\n",
       "          0.0000, -0.7830,  3.3548,  0.7604, -1.0378, -2.1095,  0.0000,  0.8052,\n",
       "         -0.0000, -0.0000, -0.0000, -2.8607, -0.0000,  1.6714,  0.0000,  2.2519,\n",
       "         -0.0000,  1.2915,  0.0000,  0.0000, -1.8682,  0.6871,  1.6112,  0.0000,\n",
       "          0.0000, -0.0000, -1.7727,  0.0000,  0.0000,  1.5898,  0.0000,  0.0000,\n",
       "          0.0000, -0.0000,  1.3165, -1.4664, -1.3171, -4.5296,  3.3987,  1.6406,\n",
       "         -0.0000,  1.7714, -2.8606, -0.6512,  0.9696,  0.0000, -1.1361, -0.2736,\n",
       "          0.0000,  0.1438, -2.1627, -0.0000, -0.0000, -0.0000,  0.0000, -1.7044,\n",
       "         -0.0000, -1.3378,  1.2594, -0.4492, -0.0000,  0.0000,  0.0000, -3.3734,\n",
       "          0.8667,  0.9227, -0.0000,  0.4423, -1.2594,  0.0554, -0.0000,  4.1033,\n",
       "         -1.8038, -1.5127,  0.6338,  0.0000, -0.0000,  2.6481, -0.0000,  0.3102,\n",
       "          0.0000,  1.5817,  1.5198, -1.5931, -2.3762,  0.9118,  0.0000, -0.0000,\n",
       "          1.3051, -0.6062, -1.8884,  0.0433, -0.5964,  1.2833,  0.0000,  0.0000,\n",
       "          0.8977, -0.8529,  0.3356,  1.3371,  0.0000, -1.1446,  0.2926,  1.5061,\n",
       "          0.0000,  1.4105, -0.0000,  2.9294, -2.3446, -0.7861,  0.0000, -1.5140,\n",
       "          0.0000, -0.2621, -0.0000, -1.7378,  1.9842,  0.8842, -1.1785,  0.6680,\n",
       "          0.0000,  0.0000, -0.2834,  1.0942, -0.2897,  0.0000, -0.0000,  1.8649,\n",
       "          0.0000,  1.0526, -1.6961,  0.8188,  1.1520, -0.0000, -0.0000,  0.0000,\n",
       "          0.0000, -0.2865,  0.0000,  0.2432,  1.8924,  0.5163,  0.0000, -0.2547,\n",
       "          3.4261,  0.0000, -0.0000,  1.2763, -0.0000, -1.3840, -0.0000, -2.5952,\n",
       "          2.3066, -0.0000, -0.7449, -2.9501, -4.3226,  0.0000,  0.0000,  1.1103,\n",
       "          1.3251,  0.0000,  0.0000,  0.0245, -0.0177, -2.6994, -0.0615, -0.6881,\n",
       "         -0.0000, -0.0000,  0.8116,  0.0000, -1.2131, -0.0000, -0.8955,  0.0000,\n",
       "         -0.7782,  0.8970,  0.7756, -0.0000, -0.0000,  0.5332, -0.0000, -0.6115,\n",
       "         -0.8401,  0.0309,  1.4379, -1.3769,  1.0640,  1.8849,  0.0000,  0.0000,\n",
       "         -1.9289, -1.7897, -2.3413, -1.7690, -0.0051,  1.1544,  1.9962,  0.0000,\n",
       "         -0.0000, -0.5412],\n",
       "        [ 0.0000, -4.0533,  1.5200,  1.9044, -0.0956,  0.5193, -2.7568, -1.4482,\n",
       "          1.9632,  0.6729, -1.7414, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "          0.0000,  1.1361,  3.1902, -0.2139, -2.0366,  0.0000, -1.3270,  0.6706,\n",
       "         -0.6754, -2.6080,  0.0000,  0.0000, -2.1349,  0.0000, -1.9637,  0.0000,\n",
       "         -0.0000, -0.0000, -1.2397,  1.8599, -1.4503, -0.9442,  0.0000, -1.0786,\n",
       "         -0.0000, -3.9760,  0.2212, -0.0253,  0.0000,  0.0000,  0.0000,  0.7502,\n",
       "          0.0000, -0.7830,  3.3548,  0.7604, -1.0378, -2.1095,  1.2361,  0.8052,\n",
       "         -2.2686, -0.0000, -0.0319, -2.8607, -0.7892,  0.0000,  0.9566,  2.2519,\n",
       "         -1.9346,  1.2915,  0.8302,  0.0000, -0.0000,  0.0000,  1.6112,  0.5487,\n",
       "          1.0147, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  1.8243,  0.0000,\n",
       "          0.0000, -0.0000,  0.0000, -1.4664, -0.0000, -0.0000,  0.0000,  1.6406,\n",
       "         -0.3028,  1.7714, -2.8606, -0.6512,  0.9696,  0.0000, -0.0000, -0.2736,\n",
       "          2.8343,  0.1438, -0.0000, -0.0000, -0.0000, -0.0000,  1.2085, -1.7044,\n",
       "         -0.9845, -0.0000,  1.2594, -0.0000, -0.0000,  2.4284,  1.5376, -3.3734,\n",
       "          0.0000,  0.9227, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "         -1.8038, -1.5127,  0.0000,  0.0444, -0.0000,  2.6481, -0.0000,  0.0000,\n",
       "          1.5805,  0.0000,  0.0000, -0.0000, -2.3762,  0.9118,  0.0000, -0.0000,\n",
       "          1.3051, -0.6062, -1.8884,  0.0000, -0.0000,  0.0000,  0.0000,  0.7339,\n",
       "          0.8977, -0.8529,  0.3356,  1.3371,  0.0000, -0.0000,  0.2926,  1.5061,\n",
       "          0.0000,  1.4105, -0.3547,  0.0000, -2.3446, -0.7861,  0.0000, -0.0000,\n",
       "          0.0000, -0.2621, -3.4947, -0.0000,  0.0000,  0.8842, -0.0000,  0.0000,\n",
       "          1.0502,  0.0000, -0.2834,  0.0000, -0.2897,  0.4500, -0.2855,  0.0000,\n",
       "          0.8423,  0.0000, -0.0000,  0.0000,  0.0000, -1.5129, -0.8605,  0.4665,\n",
       "          0.9097, -0.0000,  1.4739,  0.2432,  0.0000,  0.0000,  0.0000, -0.0000,\n",
       "          0.0000,  0.8295, -0.0000,  0.0000, -1.2302, -1.3840, -0.9655, -0.0000,\n",
       "          2.3066, -0.0000, -0.0000, -2.9501, -0.0000,  0.0000,  0.0000,  1.1103,\n",
       "          0.0000,  1.4984,  0.1252,  0.0000, -0.0000, -0.0000, -0.0615, -0.6881,\n",
       "         -2.1444, -0.0000,  0.8116,  0.0000, -1.2131, -0.4837, -0.8955,  0.0000,\n",
       "         -0.0000,  0.8970,  0.7756, -1.9777, -2.2021,  0.5332, -0.0000, -0.6115,\n",
       "         -0.0000,  0.0000,  1.4379, -0.0000,  0.0000,  0.0000,  0.7293,  0.0000,\n",
       "         -0.0000, -1.7897, -2.3413, -1.7690, -0.0051,  1.1544,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = selfembdrop(z)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_szs, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((nf for ni,nf in emb_szs))\n",
    "        n_in = n_emb\n",
    "        \n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i)) \n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self, x_cat):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x = torch.cat([x], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "model = TabularModel(emb_szs, 16, [200, 100, 50, 25, 50, 100, 200], p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(90700, 50)\n",
       "    (1): Embedding(90700, 50)\n",
       "    (2): Embedding(90700, 50)\n",
       "    (3): Embedding(90700, 50)\n",
       "    (4): Embedding(90700, 50)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=250, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.4, inplace=False)\n",
       "    (12): Linear(in_features=50, out_features=25, bias=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Dropout(p=0.4, inplace=False)\n",
       "    (16): Linear(in_features=25, out_features=50, bias=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): Dropout(p=0.4, inplace=False)\n",
       "    (20): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): Dropout(p=0.4, inplace=False)\n",
       "    (24): Linear(in_features=100, out_features=200, bias=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): Dropout(p=0.4, inplace=False)\n",
       "    (28): Linear(in_features=200, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   500  loss: 2.19426656\n",
      "epoch:  1000  loss: 2.16728044\n",
      "epoch:  1500  loss: 2.16184449\n",
      "epoch:  2000  loss: 2.15958977\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 5000\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = model(cats)\n",
    "    loss = criterion(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if (i+1)%500 == 0:\n",
    "        print(f'epoch: {(i+1):5}  loss: {loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_val = model(cats)\n",
    "    loss = criterion(y_val, y)\n",
    "print(f'CE Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = d.shape[0]\n",
    "correct = 0\n",
    "print(f'{\"MODEL OUTPUT\":26} ARGMAX  Y_TEST')\n",
    "for i in range(rows):\n",
    "    print(f'{str(y_val[i]):26} {y_val[i].argmax():^7}{y[i]:^7}')\n",
    "    if y_val[i].argmax().item() == y[i]:\n",
    "        correct += 1\n",
    "print(f'\\n{correct} out of {rows} = {100*correct/rows:.2f}% correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda70bacfca137d49cb91ba2cb89d319095"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
